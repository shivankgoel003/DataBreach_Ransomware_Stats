
---
title: "TODO"
subtitle: "TODO"
author: 
  - Shivank Goel
thanks: "Code and data are available at: https://github.com/shivankgoel003/DataBreach_Ransomware_Stats"
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(readxl)
library(here)

breach_data <- read.csv(here("data/analysis_data/breach_data.csv"))
```


# Introduction


*TODO* today's world, where everything is connected online, cybersecurity is super important. It's all about keeping our digital stuff safe from bad guys who try to steal or mess with it. Cyber threats, like hackers breaking into computer systems or stealing personal information, are becoming more common and tricky to deal with.

This paper is all about diving into the world of cybersecurity to learn about the problems we face, the trends we're seeing, and how we can protect ourselves better. We'll start by looking at some big cyber incidents that have happened recently. By understanding what happened in these incidents, we can figure out how to stop similar attacks in the future.

Then, we'll talk about the new kinds of cyber threats that are popping up all the time. From sneaky tricks like ransomware to people tricking us into giving away our info, we'll explore the different ways bad guys try to break into our digital stuff.

Finally, we'll talk about what we can do to fight back against cyber threats. This includes using smart technology, making rules and policies to keep things safe, and teaching people how to be careful online. By learning more about cyber risks and working together to stay safe, we can make sure our digital world is a lot safer for everyone.



# Data {#sec-data}

## Data Source and Collection: 

**TODO**

The study relies on datasets obtained from the provincial open databases of Alberta, accessible through the official website @alberta. Three key datasets were utilized to extract relevant variables for analysis, aiming to uncover the relationship between air quality and mortality rates in Alberta.
The analysis begins with the leading causes of death dataset for Alberta, sourced from the provincial open data portal @deaths. This dataset provides insights into mortality rates associated with various illnesses, facilitating the examination of trends related to respiratory and heart-related illnesses.
To explore potential correlations between air quality and mortality rates, the study incorporates the Air Quality Health Index (AQHI) dataset for Alberta, sourced from the provincial open data portal @air. This dataset offers comprehensive information on the AQHI across different municipalities in Alberta over multiple years.
Additionally, the study utilizes PM2.5 air pollutant concentration level data sourced from Alberta's official resources @pollution. This dataset provides detailed information on the concentration levels of PM2.5 pollutants over several years, offering valuable insights into air quality trends.
The following subsections outline the sources, collection methodologies, and data-cleaning procedures implemented to ensure the accuracy and reliability of the datasets used in the analysis. This meticulous approach ensures that the data is prepared for thorough analysis, facilitating the exploration of correlations between air quality indicators and mortality rates in Alberta.


Leading Causes of Death in Alberta Data: The disease data is found from the government of Alberta’s open data portal, and was last updated on September 22, 2023 and continues to be updated annually.  This dataset encompasses mortality data related to the top 30 common causes of death. It reports on types of diseases, causes of death, mortality denoted by total death counts, and ranking for 2000-2022.  Due to our focus on respiratory illnesses, in the leading cause of death dataset, we grouped diseases by categories. Our category of focus included filtering on illnesses like acute myocardial infarction, malignant neoplasms of the trachea, bronchus, and lung, other chronic obstructive pulmonary disease, and all other forms of chronic ischemic heart disease. Leading causes of death are measured and ranked by the top 30 most common death causes each specific year. The causes of death are classified based on the International Classification of Diseases 10th Edition.


AQHI Data: The second dataset we used is the air quality health index (AQHI) dataset found at the government of Alberta’s open data portal. This dataset contains AQHI by municipality for the years 2012-2022 and reports air quality health index, and health risk both quantitatively and qualitatively.  To use the AQHI dataset we employed simple data-cleaning practice to maintain descriptive variable names and readability. The data is measured by the percentage of hours for each year at a given air quality level, by municipality. The Air Quality Health Index is calculated based on the relative risks of a combination of common air pollutants that is known to harm human health. These pollutants are ozone (O3) at ground level, particulate matter (PM2.5), and nitrogen dioxide (NO2). Risks are defined as follows: 1-3 High Quality; 4-6 Moderate Quality; 7-9 Low Quality; 10+ Very Low Quality.

PM2.5 Data: We used the PM2.5 data set retrieved from Alberta.ca (Government of Alberta) which was last updated in April 2023. It reports on average PM2.5 concentration levels through the years 2000-2021 using a provincial average, the 10th percentile quantities, and the 90th percentile quantities, with a focus on 8 municipalities Edmonton, Fort McMurray, Grande Prairie, Lethbridge, Medicine Hat, and Red Deer respectively and lastly reports the Canadian Ambient Air Quality Standard (CAAQS) value. 

The Alberta Air Zone report @report, which is linked to our dataset, provides a detailed explanation of the measurement and processing of the PM2.5 quantity. Alberta Air Zones divides Alberta into six air zones which are aligned with Alberta’s Land-use Framework regional boundaries. Ambient air quality in Alberta is monitored at continuous air monitoring stations located within these air zones. PM2.5 quantities are taken throughout these stations across Alberta, and they measure the quantities in µg (micrograms per cubic meter of air).  

## Data Cleaning 

We used R [@citeR] and @rohan for data cleaning and processing, utilizing packages like tidyverse [@tidy] for data manipulation and janitor [@jan] for cleaning column names. Other packages used includes `ggplot2` [@ggplot], `dplyr` [@dp], `readr` [@read], `tibble` [@tib], `janitor` [@jan],`reshape2` [@reshape], `knitr` [@knit], `ggbeeswarm` [@gg], `ggrepel` [@repel], `kableExtra`[@kable], `readxl`[@readxl], `MASS`[@mass], `rstanarm`[@rstan], `modelsummary`[@model] and `here` [@here].

The raw air quality data were preprocessed to remove inconsistencies and irrelevant information. Specifically, we filtered the dataset to include observations from the years 2012 to 2021, which are relevant to our analysis. Additionally, we merged this dataset with additional information on peak pollution levels for comprehensive analysis. 
Similar to the previous datasets, the raw mortality data underwent cleaning procedures to focus on specific causes relevant to our analysis. We filtered the dataset to include observations up to 2021 and merged it with additional information on air quality for correlation analysis. 
The raw data on AQHI were filtered to include observations from the years 2011 to 2021 for consistency with other datasets. Additionally, the data were aggregated at the municipal level for further analysis. 


## Data Modifications

In this study, we constructed unique datasets by thoughtfully selecting and merging data from the Government of Alberta's open data portal, Alberta.ca, spanning the years 2012 to 2022. Our process involved merging variables from various datasets to create specific datasets tailored for model building and analysis.
One such dataset, 'cleaned_chart_data,' was created by merging variables such as causes of death, total deaths, provincial average PM2.5 levels, and CAAQS. This dataset was designed to facilitate our analysis of any significant correlations between these variables. A snapshot of this data is referenced in Table X. 
Additionally, we derived two other datasets, 'merged_data' and 'merged_heart_data,' by merging variables related to heart disease numbers, lung disease numbers, and provincial average PM2.5 values. These datasets were instrumental in examining the impact of PM2.5 on each type of illness, as previously discussed.
Overall, our methodology ensured the creation of comprehensive datasets that allowed for a detailed investigation into the relationships between PM2.5 levels and various health outcomes in Alberta. 


```{r}
breach_data %>%
  count(year) %>%
  ggplot(aes(x = as.factor(year), y = n)) +  # Convert year to factor to treat it as discrete
  geom_line(group=1) +  # Ensure geom_line treats the data as connected points
  scale_x_discrete(breaks = levels(as.factor(breach_data$year))) +  # Specify breaks at every level of the factor
  labs(title = "Cyberattacks Over Time", x = "Year", y = "Number of Attacks") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Rotate x labels to fit them all

``` 
```{r}
breach_data %>%
  count(sector) %>%
  ggplot(aes(x = reorder(sector, n), y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Cyberattacks by Sector", x = "Sector", y = "Number of Attacks")
```

```{r}
breach_data %>%
  count(country) %>%
  ggplot(aes(x = reorder(country, n), y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Cyberattacks by Country", x = "Country", y = "Number of Attacks")

```

```{r}
breach_data %>%
  count(country) %>%
  ggplot(aes(x = reorder(country, n), y = n, fill = country)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Cyberattacks by Country", x = "Country", y = "Number of Attacks") +
  theme_minimal() +
  theme(legend.position = "none") # Hides the legend

```



```{r}
breach_data %>%
  count(attack_type) %>%
  ggplot(aes(x = "", y = n, fill = attack_type)) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y") +
  labs(title = "Distribution of Attack Types")

```
Talk more about it.


And also planes (@fig-planes). (You can change the height and width, but don't worry about doing that until you have finished every other aspect of the paper - Quarto will try to make it look nice and the defaults usually work well once you have enough text.)


```{r}
breach_data %>%
  ggplot(aes(x = as.factor(year), y = impact_on_data)) +
  geom_boxplot() +
  labs(title = "Impact on Data by Year", x = "Year", y = "Impact on Data")

```
```{r}
breach_data %>%
  count(year, attack_type) %>%
  ggplot(aes(x = year, y = n, fill = attack_type)) +
  geom_area(position = 'stack') +
  labs(title = "Attack Types Over Years", x = "Year", y = "Count")

```

```{r}
library(ggplot2)
library(dplyr)

# Assuming 'overall_nature_of_attack' is your categorical variable indicating the nature/type of attack
# and 'number_of_users_affected' is a numeric variable indicating the number of users impacted

# First, let's calculate summary statistics for each nature of attack

breach_data %>%
  count(year, sector) %>%
  ggplot(aes(x = year, y = sector, fill = n)) +
  geom_tile() +
  scale_fill_gradient(low = "lightblue", high = "red") +
  labs(title = "Heatmap of Cyberattacks per Year and Sector",
       x = "Year",
       y = "Sector")

```
```{r}
breach_data %>%
  count(year, overall_nature_of_attack) %>%
  ggplot(aes(x = as.factor(year), y = n, fill = overall_nature_of_attack)) +
  geom_bar(stat = "identity") +
  labs(title = "Stacked Bar Chart of Attack Types by Year",
       x = "Year",
       y = "Number of Attacks")

```
```{r}
library(ggplot2)

ggplot(breach_data, aes(x = as.factor(year), y = number_of_users_affected)) +
  geom_boxplot() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Boxplot of Number of Users Affected Across Years",
       x = "Year",
       y = "Number of Users Affected")
```


```{r}

breach_data %>%
  filter(year == 2013) %>%
  arrange(desc(number_of_users_affected)) %>%
  head()  # This shows the top entries for 2013

breach_data %>%
  filter(year == 2014) %>%
  arrange(desc(number_of_users_affected)) %>%
  head()  # This shows the top entries for 2013

# Install and load the scales package
if (!requireNamespace("scales", quietly = TRUE)) {
  install.packages("scales")
}
library(scales)



```



```{r}
breach_data %>%
  count(country) %>%
  mutate(percentage = n / sum(n) * 100) %>%
  ggplot(aes(x = reorder(country, percentage), y = percentage)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Percentage of Total Cyberattacks by Country",
       x = "Country",
       y = "Percentage of Attacks")



```

```{r}


breach_data %>%
  mutate(impact_score = case_when(
    impact_on_data == "Low" ~ 1,
    impact_on_data == "Medium" ~ 2,
    impact_on_data == "High" ~ 3,
    TRUE ~ NA_real_
  )) %>%
  group_by(organisation_size) %>%
  summarize(average_impact = mean(impact_score, na.rm = TRUE)) %>%
  ggplot(aes(x = organisation_size, y = average_impact, fill = organisation_size)) +
  geom_col() +
  labs(title = "Average Impact Severity by Organization Size",
       x = "Organisation Size",
       y = "Average Impact Score") +
  theme_minimal()

```

```{r}
ggplot(breach_data, aes(x = number_of_users_affected)) +
  geom_density(fill = "blue", alpha = 0.5) +
  labs(title = "Density Plot of Number of Users Affected by Cyberattacks",
       x = "Number of Users Affected",
       y = "Density")

```


```{r}
ggplot(breach_data, aes(x = organisation_size, fill = organisation_size)) +
  geom_bar() +
  labs(title = "Number of Cyberattacks by Organization Size",
       x = "Organization Size",
       y = "Number of Cyberattacks") +
  theme_minimal()


```
```{r}
library(modelsummary)

logistic_model <- readRDS(file = here::here("models/restructuring_model.rds"))

modelsummary(list("Logistic Regression" = logistic_model))

```
```{r}
breach_data <- breach_data %>% mutate(row_id = row_number())

# Adjust factors in your data to match the model's training data
breach_data <- breach_data %>%
  mutate(country = factor(country, levels = levels(logistic_model$model$country)))

# Generate predictions
breach_predictions <- predict(logistic_model, newdata = breach_data, type = "response")

# Combine the predictions with the original data
breach_data <- breach_data %>% mutate(predicted_prob = breach_predictions)

# Scatter plot with jitter
ggplot(breach_data, aes(x = organisation_size, y = predicted_prob)) +
  geom_jitter(alpha = 0.3) +
  labs(x = "Organisation Size", y = "Predicted Probability of Restructuring")

```
```{r}
ggplot(breach_data, aes(x = number_of_users_affected)) +
  stat_ecdf(geom = "step") +
  labs(title = "Cumulative Distribution of Number of Users Affected",
       x = "Number of Users Affected",
       y = "Cumulative Probability")
```
```{r}
breach_data %>%
  group_by(year) %>%
  summarize(total_users_affected = sum(number_of_users_affected, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = total_users_affected)) +
  geom_line() +
  labs(title = "Number of Users Affected Over Years",
       x = "Year",
       y = "Total Number of Users Affected") +
  theme_minimal()


```

```{r}
breach_data %>%
  ggplot(aes(x = sector, y = number_of_users_affected)) +
  geom_boxplot() +
  labs(title = "Spread of Number of Users Affected Across Sectors",
       x = "Sector",
       y = "Number of Users Affected") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
breach_data %>%
  ggplot(aes(x = number_of_users_affected)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  labs(title = "Distribution of Number of Users Affected by Cyberattacks",
       x = "Number of Users Affected",
       y = "Frequency") +
  theme_minimal()

```
```{r}
breach_data %>%
  filter(year == 2013) %>%
  summarise(
    median_users = median(number_of_users_affected, na.rm = TRUE),
    iqr_users = IQR(number_of_users_affected, na.rm = TRUE),
    upper_bound = median_users + 1.5 * iqr_users
  )

breach_data %>% filter(year == 2019)

breach_data %>%
  filter(year == 2019)%>%
  summarise(
    median_users = median(number_of_users_affected, na.rm = TRUE),
    iqr_users = IQR(number_of_users_affected, na.rm = TRUE),
    upper_bound = median_users + 1.5 * iqr_users
  )

```



```{r}

```






```{r}


```

```{r}
breach_data$year <- as.numeric(as.character(breach_data$year))

# Now proceed with your data manipulation
breach_data_summary <- breach_data %>%
  mutate(country = fct_collapse(country,
                                "Other" = setdiff(unique(country), 
                                                   c("USA", "Australia", "Canada", "Global", "Japan", "UK")))) %>%
  group_by(year, country) %>%
  summarize(frequency = n(), .groups = 'drop')

# Separating the data by country for ease of plotting
country_data <- breach_data_summary %>%
  filter(country != "USA")

usa_data <- breach_data_summary %>%
  filter(country == "USA") %>%
  arrange(year)

# Ensure year is numeric for the line plot
breach_data_summary$year <- as.numeric(as.character(breach_data_summary$year))

# Recalculate max frequencies if necessary
max_usa_freq <- max(usa_data$frequency, na.rm = TRUE)
max_other_freq <- max(country_data$frequency, na.rm = TRUE)

# Determine scale factor for secondary axis
scale_factor <- max_other_freq / max_usa_freq
breach_data_summary$year <- as.numeric(as.character(breach_data_summary$year))
breach_data_summary <- breach_data_summary %>% filter(!is.na(year))

# Plot
# Define a color palette
formal_palette <- c("Australia" = "#4878D0", "Canada" = "#6ACC65", "Global" = "#D65F5F", 
                    "Japan" = "#B47CC7", "UK" = "#C4AD66", "Other" = "#77BEDB")

# Plot
gg <- ggplot() +
  # Add bars for all countries except USA
  geom_bar(data = country_data, aes(x = year, y = frequency, fill = country), stat = "identity", position = position_stack()) +
  # Add line for USA
  geom_line(data = usa_data, aes(x = year, y = frequency * scale_factor, group = 1), color = "#4D4D4D", size = 1) +
  # Define the primary y-axis with the secondary axis for the USA
  scale_y_continuous(
    name = "Frequency of Cyber Attacks (Other Countries)", 
    limits = c(0, max_other_freq * 1.1),  # Set limits for better control, slightly above max to ensure space for the line
    sec.axis = sec_axis(~ . / scale_factor, name = "Frequency of USA Cyber Attacks", labels = scales::comma)
  ) +
  # Set breaks for the x-axis to unique years
  scale_x_continuous(breaks = sort(unique(breach_data_summary$year))) +
  # Apply the formal color palette
  scale_fill_manual(values = formal_palette) +
  labs(title = "Overview of Cyber Attacks by Year and Country",
       subtitle = "Bar plots for countries; line plot for USA") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), # Rotate x-axis labels
        legend.position = "bottom", # Adjust the position of the legend
        plot.title = element_text(hjust = 0.5), # Center the plot title
        plot.subtitle = element_text(hjust = 0.5)) # Center the plot subtitle


# Print the plot
print(gg)

```

```{r}


```









\newpage


# References
