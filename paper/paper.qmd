
---
title: "Respiratory-Related Mortality Rates Show A Positive Correlation With Increasing Air pollution"
subtitle: "Based on Data Collected From The Province Of Alberta"
author: 
  - Vanshika Vanshika
  - Shivank Goel
  - Navya Hooda
thanks: "Code and data are available at: https://github.com/shivankgoel003/Mortality-in-Alberta."
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(readxl)
library(here)

breach_data <- read.csv(here("data/analysis_data/breach_data.csv"))
```


# Introduction

The impact of air pollution on human health has increasingly become a global concern, with respiratory illnesses being a significant consequence of poor air quality. The National Library of Medicine reports that approximately 4 million people die prematurely each year from chronic respiratory diseases linked to air pollution @ncbi. 
The World Health Organization also suggests that air pollution poses a risk for all-cause mortality and specific diseases, especially the exposure to air pollution is strongly linked with outcomes such as stroke, ischemic heart disease, chronic obstructive pulmonary disease, lung cancer, pneumonia, and cataracts @who. 

The pollutants most significantly concerning for public health, are particulate matter (PM), carbon monoxide (CO), ozone (O3), nitrogen dioxide (NO2), and sulfur dioxide (SO2). Of these, fine particulate matter is particularly worrisome due to its ability to penetrate the lungs deeply, enter the bloodstream, and reach organs, leading to systemic damage to tissues and cells. 

Specifically, PM2.5 is of greater concern due to its harmful properties and dangers. PM2.5 is airborne particulate matter (PM2.5) and is not a single pollutant but rather a mixture of many chemical species and aerosols. PM2.5 is associated with the greatest proportion of adverse health effects related to air pollution, both in the United States and worldwide. Long-term (months to years) exposure to PM2.5 has been linked to premature death, particularly in people who have chronic heart or lung diseases, and reduced lung function growth in children @pm. 


In this paper, we analyze data from Alberta, Canada, and aim to explore the estimand of the number of deaths that can be correlated to the Air Quality Health Index (AQHI) and PM2.5 quantities. Specifically, we assess how AQHI relates to the prevalence of respiratory and cardiac illnesses in this specific region, focusing on four major types: Chronic Obstructive Pulmonary Disease, Ischemic Heart Disease, Acute Myocardial Infarction, and Lung Cancer. We use the mortality rate in Alberta data to select respiratory-related illnesses and air quality health index data for Alberta through their provincial open data portal. To analyze respiratory illnesses, we use air pollution measured through the AQHI and specifically the particulate matter 2.5 (PM2.5) air pollutant as a predictor of mortality related to respiratory illnesses. In total, we will be assessing the overall relationship between AQHI and respiratory illnesses, the relationship between PM2.5 and respiratory illnesses, as well as the relevance of PM2.5 levels in lung and heart disease values as a means to draw any notable significance of PM2.5 in different types of illnesses.

Using negative binomial regression, this study seeks to uncover trends and correlations between AQHI and the prevalence of respiratory illnesses. Negative binomial regression is a type of generalized linear model (GLM) that is specifically designed to model count data. In our paper, we are using it to predict the number of deaths, based on the year and illness, for the pollutants present in the air.
By analyzing this relationship, we provide valuable insights that can inform policymakers, healthcare professionals, and the public about the impact of air pollution on respiratory health in Alberta. This research aims to contribute to a better understanding of the health effects of air pollution and to support the development of targeted strategies for air quality improvement and public health protection in Alberta. Our analytical framework explores whether the variables assessed seem to correlate, if at all, to the total deaths in respiratory-related illnesses through 2012-2022. 


This paper is organized as follows: In the Data section, we outline the sources of three different datasets, detail the data-cleaning processes applied to each dataset, and describe any data merging procedures used to prepare the data for input into various models. The Results section focuses on analyzing trends and correlations between AQHI, PM2.5, and respiratory and cardiac illnesses. Additionally, we discuss the trends and patterns identified by our model, along with the correlation analysis between these variables. In the Discussion section, we present our overall findings, discuss any biases and weaknesses in the data that may have influenced these findings, and explain our approach to analyzing these limitations. 
 
Overall, this research has the potential to inform public health strategies and interventions aimed at reducing respiratory illnesses and improving overall health in Alberta and beyond. 


# Data {#sec-data}

## Data Source and Collection: 

The study relies on datasets obtained from the provincial open databases of Alberta, accessible through the official website @alberta. Three key datasets were utilized to extract relevant variables for analysis, aiming to uncover the relationship between air quality and mortality rates in Alberta.
The analysis begins with the leading causes of death dataset for Alberta, sourced from the provincial open data portal @deaths. This dataset provides insights into mortality rates associated with various illnesses, facilitating the examination of trends related to respiratory and heart-related illnesses.
To explore potential correlations between air quality and mortality rates, the study incorporates the Air Quality Health Index (AQHI) dataset for Alberta, sourced from the provincial open data portal @air. This dataset offers comprehensive information on the AQHI across different municipalities in Alberta over multiple years.
Additionally, the study utilizes PM2.5 air pollutant concentration level data sourced from Alberta's official resources @pollution. This dataset provides detailed information on the concentration levels of PM2.5 pollutants over several years, offering valuable insights into air quality trends.
The following subsections outline the sources, collection methodologies, and data-cleaning procedures implemented to ensure the accuracy and reliability of the datasets used in the analysis. This meticulous approach ensures that the data is prepared for thorough analysis, facilitating the exploration of correlations between air quality indicators and mortality rates in Alberta.


Leading Causes of Death in Alberta Data: The disease data is found from the government of Alberta’s open data portal, and was last updated on September 22, 2023 and continues to be updated annually.  This dataset encompasses mortality data related to the top 30 common causes of death. It reports on types of diseases, causes of death, mortality denoted by total death counts, and ranking for 2000-2022.  Due to our focus on respiratory illnesses, in the leading cause of death dataset, we grouped diseases by categories. Our category of focus included filtering on illnesses like acute myocardial infarction, malignant neoplasms of the trachea, bronchus, and lung, other chronic obstructive pulmonary disease, and all other forms of chronic ischemic heart disease. Leading causes of death are measured and ranked by the top 30 most common death causes each specific year. The causes of death are classified based on the International Classification of Diseases 10th Edition.


AQHI Data: The second dataset we used is the air quality health index (AQHI) dataset found at the government of Alberta’s open data portal. This dataset contains AQHI by municipality for the years 2012-2022 and reports air quality health index, and health risk both quantitatively and qualitatively.  To use the AQHI dataset we employed simple data-cleaning practice to maintain descriptive variable names and readability. The data is measured by the percentage of hours for each year at a given air quality level, by municipality. The Air Quality Health Index is calculated based on the relative risks of a combination of common air pollutants that is known to harm human health. These pollutants are ozone (O3) at ground level, particulate matter (PM2.5), and nitrogen dioxide (NO2). Risks are defined as follows: 1-3 High Quality; 4-6 Moderate Quality; 7-9 Low Quality; 10+ Very Low Quality.

PM2.5 Data: We used the PM2.5 data set retrieved from Alberta.ca (Government of Alberta) which was last updated in April 2023. It reports on average PM2.5 concentration levels through the years 2000-2021 using a provincial average, the 10th percentile quantities, and the 90th percentile quantities, with a focus on 8 municipalities Edmonton, Fort McMurray, Grande Prairie, Lethbridge, Medicine Hat, and Red Deer respectively and lastly reports the Canadian Ambient Air Quality Standard (CAAQS) value. 

The Alberta Air Zone report @report, which is linked to our dataset, provides a detailed explanation of the measurement and processing of the PM2.5 quantity. Alberta Air Zones divides Alberta into six air zones which are aligned with Alberta’s Land-use Framework regional boundaries. Ambient air quality in Alberta is monitored at continuous air monitoring stations located within these air zones. PM2.5 quantities are taken throughout these stations across Alberta, and they measure the quantities in µg (micrograms per cubic meter of air).  

## Data Cleaning 

We used R [@citeR] and @rohan for data cleaning and processing, utilizing packages like tidyverse [@tidy] for data manipulation and janitor [@jan] for cleaning column names. Other packages used includes `ggplot2` [@ggplot], `dplyr` [@dp], `readr` [@read], `tibble` [@tib], `janitor` [@jan],`reshape2` [@reshape], `knitr` [@knit], `ggbeeswarm` [@gg], `ggrepel` [@repel], `kableExtra`[@kable], `readxl`[@readxl], `MASS`[@mass], `rstanarm`[@rstan], `modelsummary`[@model] and `here` [@here].

The raw air quality data were preprocessed to remove inconsistencies and irrelevant information. Specifically, we filtered the dataset to include observations from the years 2012 to 2021, which are relevant to our analysis. Additionally, we merged this dataset with additional information on peak pollution levels for comprehensive analysis. 
Similar to the previous datasets, the raw mortality data underwent cleaning procedures to focus on specific causes relevant to our analysis. We filtered the dataset to include observations up to 2021 and merged it with additional information on air quality for correlation analysis. 
The raw data on AQHI were filtered to include observations from the years 2011 to 2021 for consistency with other datasets. Additionally, the data were aggregated at the municipal level for further analysis. 


## Data Modifications

In this study, we constructed unique datasets by thoughtfully selecting and merging data from the Government of Alberta's open data portal, Alberta.ca, spanning the years 2012 to 2022. Our process involved merging variables from various datasets to create specific datasets tailored for model building and analysis.
One such dataset, 'cleaned_chart_data,' was created by merging variables such as causes of death, total deaths, provincial average PM2.5 levels, and CAAQS. This dataset was designed to facilitate our analysis of any significant correlations between these variables. A snapshot of this data is referenced in Table X. 
Additionally, we derived two other datasets, 'merged_data' and 'merged_heart_data,' by merging variables related to heart disease numbers, lung disease numbers, and provincial average PM2.5 values. These datasets were instrumental in examining the impact of PM2.5 on each type of illness, as previously discussed.
Overall, our methodology ensured the creation of comprehensive datasets that allowed for a detailed investigation into the relationships between PM2.5 levels and various health outcomes in Alberta. 


```{r}
breach_data %>%
  count(year) %>%
  ggplot(aes(x = as.factor(year), y = n)) +  # Convert year to factor to treat it as discrete
  geom_line(group=1) +  # Ensure geom_line treats the data as connected points
  scale_x_discrete(breaks = levels(as.factor(breach_data$year))) +  # Specify breaks at every level of the factor
  labs(title = "Cyberattacks Over Time", x = "Year", y = "Number of Attacks") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Rotate x labels to fit them all

``` 
```{r}
breach_data %>%
  count(sector) %>%
  ggplot(aes(x = reorder(sector, n), y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Cyberattacks by Sector", x = "Sector", y = "Number of Attacks")
```

```{r}
breach_data %>%
  count(country) %>%
  ggplot(aes(x = reorder(country, n), y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Cyberattacks by Country", x = "Country", y = "Number of Attacks")

```

```{r}
breach_data %>%
  count(country) %>%
  ggplot(aes(x = reorder(country, n), y = n, fill = country)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Cyberattacks by Country", x = "Country", y = "Number of Attacks") +
  theme_minimal() +
  theme(legend.position = "none") # Hides the legend

```



```{r}
breach_data %>%
  count(attack_type) %>%
  ggplot(aes(x = "", y = n, fill = attack_type)) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y") +
  labs(title = "Distribution of Attack Types")

```
Talk more about it.


And also planes (@fig-planes). (You can change the height and width, but don't worry about doing that until you have finished every other aspect of the paper - Quarto will try to make it look nice and the defaults usually work well once you have enough text.)


```{r}
breach_data %>%
  ggplot(aes(x = as.factor(year), y = impact_on_data)) +
  geom_boxplot() +
  labs(title = "Impact on Data by Year", x = "Year", y = "Impact on Data")

```
```{r}
breach_data %>%
  count(year, attack_type) %>%
  ggplot(aes(x = year, y = n, fill = attack_type)) +
  geom_area(position = 'stack') +
  labs(title = "Attack Types Over Years", x = "Year", y = "Count")

```

```{r}
library(ggplot2)
library(dplyr)

# Assuming 'overall_nature_of_attack' is your categorical variable indicating the nature/type of attack
# and 'number_of_users_affected' is a numeric variable indicating the number of users impacted

# First, let's calculate summary statistics for each nature of attack

breach_data %>%
  count(year, sector) %>%
  ggplot(aes(x = year, y = sector, fill = n)) +
  geom_tile() +
  scale_fill_gradient(low = "lightblue", high = "red") +
  labs(title = "Heatmap of Cyberattacks per Year and Sector",
       x = "Year",
       y = "Sector")

```
```{r}
breach_data %>%
  count(year, overall_nature_of_attack) %>%
  ggplot(aes(x = as.factor(year), y = n, fill = overall_nature_of_attack)) +
  geom_bar(stat = "identity") +
  labs(title = "Stacked Bar Chart of Attack Types by Year",
       x = "Year",
       y = "Number of Attacks")

```
```{r}
library(ggplot2)

ggplot(breach_data, aes(x = as.factor(year), y = number_of_users_affected)) +
  geom_boxplot() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Boxplot of Number of Users Affected Across Years",
       x = "Year",
       y = "Number of Users Affected")
```


```{r}

breach_data %>%
  filter(year == 2013) %>%
  arrange(desc(number_of_users_affected)) %>%
  head()  # This shows the top entries for 2013

breach_data %>%
  filter(year == 2014) %>%
  arrange(desc(number_of_users_affected)) %>%
  head()  # This shows the top entries for 2013

# Install and load the scales package
if (!requireNamespace("scales", quietly = TRUE)) {
  install.packages("scales")
}
library(scales)



```



```{r}
breach_data %>%
  count(country) %>%
  mutate(percentage = n / sum(n) * 100) %>%
  ggplot(aes(x = reorder(country, percentage), y = percentage)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Percentage of Total Cyberattacks by Country",
       x = "Country",
       y = "Percentage of Attacks")



```

```{r}


breach_data %>%
  mutate(impact_score = case_when(
    impact_on_data == "Low" ~ 1,
    impact_on_data == "Medium" ~ 2,
    impact_on_data == "High" ~ 3,
    TRUE ~ NA_real_
  )) %>%
  group_by(organisation_size) %>%
  summarize(average_impact = mean(impact_score, na.rm = TRUE)) %>%
  ggplot(aes(x = organisation_size, y = average_impact, fill = organisation_size)) +
  geom_col() +
  labs(title = "Average Impact Severity by Organization Size",
       x = "Organisation Size",
       y = "Average Impact Score") +
  theme_minimal()

```

```{r}
ggplot(breach_data, aes(x = number_of_users_affected)) +
  geom_density(fill = "blue", alpha = 0.5) +
  labs(title = "Density Plot of Number of Users Affected by Cyberattacks",
       x = "Number of Users Affected",
       y = "Density")

```


```{r}
ggplot(breach_data, aes(x = organisation_size, fill = organisation_size)) +
  geom_bar() +
  labs(title = "Number of Cyberattacks by Organization Size",
       x = "Organization Size",
       y = "Number of Cyberattacks") +
  theme_minimal()


```
```{r}
library(modelsummary)

logistic_model <- readRDS(file = here::here("models/restructuring_model.rds"))

modelsummary(list("Logistic Regression" = logistic_model))

```
```{r}
breach_data <- breach_data %>% mutate(row_id = row_number())

# Adjust factors in your data to match the model's training data
breach_data <- breach_data %>%
  mutate(country = factor(country, levels = levels(logistic_model$model$country)))

# Generate predictions
breach_predictions <- predict(logistic_model, newdata = breach_data, type = "response")

# Combine the predictions with the original data
breach_data <- breach_data %>% mutate(predicted_prob = breach_predictions)

# Scatter plot with jitter
ggplot(breach_data, aes(x = organisation_size, y = predicted_prob)) +
  geom_jitter(alpha = 0.3) +
  labs(x = "Organisation Size", y = "Predicted Probability of Restructuring")

```
```{r}
ggplot(breach_data, aes(x = number_of_users_affected)) +
  stat_ecdf(geom = "step") +
  labs(title = "Cumulative Distribution of Number of Users Affected",
       x = "Number of Users Affected",
       y = "Cumulative Probability")
```
```{r}
breach_data %>%
  group_by(year) %>%
  summarize(total_users_affected = sum(number_of_users_affected, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = total_users_affected)) +
  geom_line() +
  labs(title = "Number of Users Affected Over Years",
       x = "Year",
       y = "Total Number of Users Affected") +
  theme_minimal()


```

```{r}
breach_data %>%
  ggplot(aes(x = sector, y = number_of_users_affected)) +
  geom_boxplot() +
  labs(title = "Spread of Number of Users Affected Across Sectors",
       x = "Sector",
       y = "Number of Users Affected") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
breach_data %>%
  ggplot(aes(x = number_of_users_affected)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  labs(title = "Distribution of Number of Users Affected by Cyberattacks",
       x = "Number of Users Affected",
       y = "Frequency") +
  theme_minimal()

```
```{r}
breach_data %>%
  filter(year == 2013) %>%
  summarise(
    median_users = median(number_of_users_affected, na.rm = TRUE),
    iqr_users = IQR(number_of_users_affected, na.rm = TRUE),
    upper_bound = median_users + 1.5 * iqr_users
  )

breach_data %>% filter(year == 2019)

breach_data %>%
  filter(year == 2019)%>%
  summarise(
    median_users = median(number_of_users_affected, na.rm = TRUE),
    iqr_users = IQR(number_of_users_affected, na.rm = TRUE),
    upper_bound = median_users + 1.5 * iqr_users
  )

```

```{r}
ggplot(breach_data, aes(x = log1p(number_of_users_affected))) +
  geom_density() +
  labs(x = "Log(Number of Users Affected + 1)", y = "Density") +
  theme_minimal()
```

```{r}
library(modelsummary)
lrmodel <- readRDS(file = here::here("models/lrmodel.rds"))

modelsummary(list("Linear Regression" = lrmodel))

ggplot(yearly_attacks, aes(x = year, y = number_of_attacks)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  theme_minimal() +
  labs(title = "Trend of Cyberattacks Over Years",
       x = "Year", y = "Number of Attacks")

library(broom)
# Augment the data with model diagnostics
augmented_data <- augment(lrmodel)

# Plot 1: Histogram of Residuals
ggplot(augmented_data, aes(x = .resid)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Histogram of Residuals", y = "Number of Occurrences", x = "Residuals")

# Plot 2: Residuals vs. Predictor Variable (year)
ggplot(augmented_data, aes(x = year, y = .resid)) +
  geom_point(color = "red") +
  geom_hline(yintercept = 0, linetype = "dotted", color = "grey") +
  theme_minimal() +
  labs(title = "Residuals vs. Year", y = "Residuals", x = "Year")

# Plot 3: Residuals vs. Fitted Values
ggplot(augmented_data, aes(x = .fitted, y = .resid)) +
  geom_point(color = "green") +
  geom_hline(yintercept = 0, linetype = "dotted", color = "grey") +
  theme_minimal() +
  labs(title = "Residuals vs. Fitted Values", y = "Residuals", x = "Fitted Values")

# Plot 4: Actual vs. Predicted Values
ggplot(augmented_data, aes(x = number_of_users_affected, y = .fitted)) +
  geom_point(color = "purple") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  theme_minimal() +
  labs(title = "Actual vs. Predicted Values", y = "Predicted", x = "Actual")

```


```{r}
liner_model <- readRDS(file = here::here("models/linear_model.rds"))


# Compare the actual outcomes with simulations from the posterior distribution
pp_check(liner_model) +
  theme_classic() +
  theme(legend.position = "bottom")
```

```{r}
augmented_data <- augment(sensitivity_model)

# Residuals vs. Fitted Values
ggplot(augmented_data, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dotted") +
  theme_minimal() +
  labs(title = "Residuals vs. Fitted Values", x = "Fitted Values", y = "Residuals")

# Actual vs. Predicted Values
ggplot(augmented_data, aes(x = number_of_users_affected, y = .fitted)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  theme_minimal() +
  labs(title = "Actual vs. Predicted Values", x = "Actual Values", y = "Predicted Values")
```




```{r}
cyberattack_neg_binomial <- readRDS(file = here::here("models/cyberattack_neg_binomial.rds"))
cyberattack_poisson <- readRDS(file = here::here("models/cyberattack_poisson.rds"))

#### Posterior Predictive Checks ####
# Poisson model
pp_poisson <- pp_check(cyberattack_poisson) + 
  theme(legend.position = "bottom") +
  labs(title = "Posterior Predictive Check for Poisson Model")

# Negative Binomial model
pp_neg_binomial <- pp_check(cyberattack_neg_binomial) + 
  theme(legend.position = "bottom") +
  labs(title = "Posterior Predictive Check for Negative Binomial Model")

# Display the plots
print(pp_poisson)
print(pp_neg_binomial)

```

```{r}
library(ggplot2)
library(dplyr)
library(forcats) # For fct_collapse

# Assuming breach_data is already loaded and has columns named 'year', 'country', and 'frequency'
# and frequency is already calculated

# Prepare the dataset
breach_data_summary <- breach_data %>%
  mutate(country = fct_collapse(country,
                                "Other countries" = setdiff(unique(country), 
                                                              c("USA", "Australia", "Canada", "Global", "Japan", "UK")))) %>%
  group_by(year, country) %>%
  summarize(frequency = n(), .groups = 'drop')

# Separating the data by country for ease of plotting
country_data <- breach_data_summary %>%
  filter(country %in% c("Australia", "Canada", "Global", "Japan", "UK", "Other countries"))

usa_data <- breach_data_summary %>%
  filter(country == "USA")

# Find the max frequency for USA to scale the secondary axis accordingly
max_usa_frequency <- max(usa_data$frequency)

# Define a color palette
color_palette <- c("Australia" = "#1b9e77", "Canada" = "#d95f02", "Global" = "#7570b3", "Japan" = "#e7298a", "UK" = "#66a61e", "Other countries" = "#e6ab02", "USA" = "#666666")

# Plot
gg <- ggplot() +
  # Add bar plots for all countries except USA
  geom_bar(data = country_data, aes(x = as.factor(year), y = frequency, fill = country), stat = "identity", position = "dodge") +
  # Add line for USA with points
  geom_line(data = usa_data, aes(x = as.factor(year), y = frequency), color = color_palette["USA"]) +
  geom_point(data = usa_data, aes(x = as.factor(year), y = frequency), color = color_palette["USA"]) +
  # Add primary y-axis for "Other countries"
  scale_y_continuous(name = "Frequency of Cyber Attacks (Other Countries)", labels = scales::comma) +
  # Add secondary y-axis for USA
  scale_y_continuous(name = "Frequency of Cyber Attacks (USA)",
                     sec.axis = sec_axis(~ . * max_usa_frequency / max(breach_data_summary$frequency), name = "USA", labels = scales::comma)) +
  # Set the breaks for x-axis to every year level
  scale_x_discrete(breaks = levels(as.factor(breach_data$year))) +
  scale_fill_manual(values = color_palette) +
  labs(title = "Overview of Cyber Attacks by Year and Country",
       subtitle = "Bar plots for countries; line plot for USA scaled on secondary axis") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), # Rotate x-axis labels
        legend.position = "bottom") # Adjust the position of the legend

# Print the plot
print(gg)

# Save the plot
ggsave("cyber_attacks_overview.png", plot = gg, width = 12, height = 8, dpi = 300)

```


```{r}

library(ggplot2)
library(dplyr)
library(forcats) # For fct_collapse to group levels into "Other countries"

# Assuming breach_data is already loaded and has columns named 'year', 'country', and 'frequency'
# and frequency is already calculated (e.g., it is not just a raw count of rows)

# Prepare the dataset
breach_data_summary <- breach_data %>%
  mutate(country = fct_collapse(country,
                                "Other countries" = setdiff(unique(country), 
                                                              c("USA", "Australia", "Canada", "Global", "Japan", "UK")))) %>%
  group_by(year, country) %>%
  summarize(frequency = n(), .groups = 'drop')

# Separating the data by country for ease of plotting
country_data <- breach_data_summary %>%
  filter(country %in% c("Australia", "Canada", "Global", "Japan", "UK", "Other countries"))

usa_data <- breach_data_summary %>%
  filter(country == "USA")

# Find the max frequency for USA to scale the secondary axis accordingly
max_usa_frequency <- max(usa_data$frequency)

# Plot
gg <- ggplot() +
  # Add bar plots for all countries except USA
  geom_bar(data = country_data, aes(x = as.factor(year), y = frequency, fill = country), stat = "identity", position = "dodge") +
  # Add line for USA with points
  geom_line(data = usa_data, aes(x = as.factor(year), y = frequency), color = "blue") +
  geom_point(data = usa_data, aes(x = as.factor(year), y = frequency), color = "blue") +
  # Add primary y-axis for "Other countries"
  scale_y_continuous(name = "Frequency of Cyber Attacks (Other Countries)") +
  # Add secondary y-axis for USA
  scale_y_continuous(name = "Frequency of Cyber Attacks (USA)",
                     sec.axis = sec_axis(~ . * max_usa_frequency / max(breach_data_summary$frequency), name = "USA")) +
  # Set the breaks for x-axis to every year level
  scale_x_discrete(breaks = levels(as.factor(breach_data$year))) +
  scale_fill_manual(values = c("Australia" = "color1", "Canada" = "color2", "Global" = "color3", "Japan" = "color4", "UK" = "color5", "Other countries" = "color6")) +
  labs(title = "Overview of Cyber Attacks by Year and Country",
       subtitle = "Bar plots for countries; line plot for USA scaled on secondary axis") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), # Rotate x-axis labels
        legend.position = "bottom") # Adjust the position of the legend

# Print the plot
print(gg)

```

Make sure to replace "color1", "color2", etc., with the actual colors you want for the bars. Adjust the names of the columns if needed, based on your dataset. The `scale_y_continuous` for the USA might need to be adjusted based on the actual frequencies in your dataset. The `sec.axis` function is used to create the secondary

```


\newpage


# References
